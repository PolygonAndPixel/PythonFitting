{
  "name": "Pythonfitting",
  "tagline": "",
  "body": "# PythonFitting\r\nIn this script I am using different calculations to fit some data to a function.\r\nThese fits are solutions some tasks of the course **Modellierung I** by Prof. Michael Wand\r\nat the Johannes Gutenberg-University in Mainz. \r\nI give no guarantee for correctness, style or efficiency (which is obvious if \r\nyou look at my matrix-matrix multiplication for instance). However if you want\r\nto see some basic implementations, feel free to take a look, copy some code and\r\nplay with it.\r\nIf you want to get to know something about style, please visit \r\n[PEP8](https://www.python.org/dev/peps/pep-0008/) or use at least\r\n[pylint](https://www.pylint.org/) which has some nice GUI interfaces like \r\n[Spyder IDE](https://github.com/spyder-ide/spyder).\r\nDo not hesitate to contact me if you find any mistakes or simply open an issue \r\nor make a pull request.\r\n\r\n## Usage\r\nSimply type\r\n\r\n`python interpol.py [-t task]`\r\n\r\nto get started. There are different options for most plots:\r\n\r\n| Options | Description |\r\n|---------|-------------|\r\n| '-t', '--task' | type=int, required=True, default=1, choose the task (and therefore the data and fit) to solve.|\r\n|'-d', '--degree'| type=int, required=False, default=3, sets the degree to use for polynomial least square fitting.|\r\n|'-f', '--datafile'| type=str, required=False, default='dataPCPC', the datafile for tasks 4, 5 and 6. |\r\n|'-p', '--predict'| type=int, required=False, default=0, the desired prediction for the dataset. Should be bigger than 0.|\r\n|'-past', type=int| required=False, default=0, the desired look into the past for the dataset. Should be bigger than 0.|\r\n|'-v', '--verbose'| type=int, required=False, default=0, set verbositiy from 0 to 2.|\r\n|'-h', '--help'| Show the help message.|\r\n\r\nThe different tasks are:\r\n\r\n* Task 1: Linear interpolation with two datapoints.\r\n* Task 2: Polynomial interpolation by solving a Vandermonde matrix with degree 3 on 4 datapoints.\r\n* Task 3: Polynomial interpolation with least squares with degree 4 and 5 on 4 datapoints.\r\n* Task 4: Linear fitting with least squares on a dataset 'dataPCPC' or any\r\nother specified with '-f'.\r\n* Task 5: Polynomial fitting with least squares on a dataset 'dataPCPC' or any\r\nother specified with '-f'.\r\n* Task 6: Exponential fitting with least squares on a dataset 'dataPCPC' or any\r\nother specified with '-f'.\r\n* Task 7: L1-Norm fitting on a dataset 'dataPCPC' or any other specified with '-f'.\r\nHere some datapoints are exchanged with outliers.\r\n* Task 8: M-Estimators approach to fit on a dataset 'dataPCPC' or any other specified with '-f'.\r\nHere some datapoints are exchanged with outliers.\r\n\r\n## The fits\r\n\r\n### M-Estimator\r\nThose are estimators which obtain the minima of sums of functions of the data.\r\nLeast squares estimators are a special case of M-estimators.\r\nA good summary of such functions can be found [here](http://research.microsoft.com/en-us/um/people/zhang/INRIA/Publis/Tutorial-Estim/node24.html).\r\n\r\n### Linear interpolation between two datapoints\r\nLinear interpolation between two points is the easiest you can do. You just\r\ndefine a function:\r\n\r\n![Imgur](http://i.imgur.com/3UoBvJB.png)\r\n\r\nNow you insert your two datapoints for *x* and *y* and solve the equation system\r\nfor *c* and *m*.\r\n\r\n### Polynomial interpolation\r\nIn a polynomial interpolation we define a polynomial of degree *k* to fit the \r\ndataset. Usually one starts with low degrees and gets higher if the fit is not\r\ngood. High degrees lead to high oscillations of the function which tries to fit\r\nall the points which is usually not desired. Therefore I restricted the degree\r\nto the amount of available datapoints in my implementation.\r\nIn order to get the coefficients of the polynomial one has to create a\r\nVandermonde matrix of the *x*-values of the datapoints, multiply it with a vector\r\nof the unknown coefficients and these should equal the *y*-values of the \r\ndatapoints:\r\n\r\n![Imgur](http://i.imgur.com/aHpROMz.png)\r\n\r\nThis approach can be easily disturbed by outliers and leads to fits with an high\r\ndegree since the function tries to fit every datapoint.\r\n\r\n### Linear interpolation with least squares\r\nLeast squares allows some error in the fit but it also minimizes the distance between\r\nthe fit and the datapoints. This leads to functions with lower degrees than a \r\nsimple interpolation and outliers have a smaller impact. Here we define a linear\r\nfunction again and solve for the slope using the average values of *x* and *y*:\r\n\r\n![Imgur](http://i.imgur.com/oSb8GvY.png)\r\n\r\nThe intersection with the *y*-axes can be calculated with:\r\n\r\n![Imgur](http://i.imgur.com/9SRLfXA.png)\r\n\r\n\r\n### Polynomial interpolation with least \r\nWe define again some kind of polynomial but this time our Vandermonde matrix looks\r\na bit different since we try to fit an average curve.\r\nIn this approach we try to solve following equation system:\r\n\r\n![Imgur](http://i.imgur.com/Ac8uibi.png)\r\n\r\nFor more details on how to derive this system I recommend [mathworld wolfram](http://mathworld.wolfram.com/LeastSquaresFittingPolynomial.html) and [neutrium](https://neutrium.net/mathematics/least-squares-fitting-of-a-polynomial/).\r\n\r\n### Exponential interpolation with least squares\r\nThis is a short summary of an explanation at [mathworld wolfram](http://mathworld.wolfram.com/LeastSquaresFittingExponential.html).\r\nFor an exponential interpolation the data has to fit a function with the form:\r\n\r\n![Imgur](http://i.imgur.com/lJNKmZc.png)\r\n\r\nBy taking the logarithm we can solve for *a* and *b* and least squares gives:\r\n\r\n![Imgur](http://i.imgur.com/UDp2T2a.png)\r\n\r\n### L1-Norm interpolation \r\nL1-Norm interpolation gives weights to the datapoints which reduces the impact\r\nof outliers even further. I implemented an iterative approach which redefines \r\nthe weights until it converges. If you try the code you can use an higher verbosity-level\r\nto see that rarely more than three iterations are needed. In this approach we\r\ndefine again a Vandermonde matrix *X*:\r\n\r\n![Imgur](http://i.imgur.com/WKJzs2s.jpg)\r\n\r\nWe also have our array *Y* of *y*-values:\r\n\r\n![Imgur](http://i.imgur.com/KtZyuGA.png)\r\n\r\nIn addition we define a weight matrix *W* with the weights on the diagonal and \r\nthe other entries are zeros:\r\n\r\n![Imgur](http://i.imgur.com/fM3daZw.png)\r\n\r\nWe use *a* as our array of coefficients again. Now we solve the following equation:\r\n\r\n![Imgur](http://i.imgur.com/vRbxfN4.jpg)\r\n\r\nIn this implementation we set the weights to some arbitrary numbers between 0 and\r\n1 and update the weights in each iteration with:\r\n\r\n![Imgur](http://i.imgur.com/9OAPmM6.jpg)\r\n\r\nwhere *t-1* means the new fit at position *x_i* from the iteration before.",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}